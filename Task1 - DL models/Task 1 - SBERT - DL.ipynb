{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\reshm\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\reshm\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\reshm\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\reshm\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\reshm\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\reshm\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\reshm\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\reshm\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\reshm\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\reshm\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\reshm\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\reshm\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in c:\\users\\reshm\\anaconda3\\lib\\site-packages (0.0.24)\n",
      "Requirement already satisfied: textsearch in c:\\users\\reshm\\anaconda3\\lib\\site-packages (from contractions) (0.0.17)\n",
      "Requirement already satisfied: Unidecode in c:\\users\\reshm\\anaconda3\\lib\\site-packages (from textsearch->contractions) (1.1.1)\n",
      "Requirement already satisfied: pyahocorasick in c:\\users\\reshm\\anaconda3\\lib\\site-packages (from textsearch->contractions) (1.4.0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "#from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "import logging\n",
    "import urllib\n",
    "import sys\n",
    "import os\n",
    "import zipfile\n",
    "from os.path import join, exists\n",
    "\n",
    "# 1.Importing all the libraries\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from scipy import stats\n",
    "from scipy.stats import randint\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation \n",
    "from nltk.corpus import stopwords \n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn import neighbors\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, precision_recall_curve\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D, CuDNNLSTM, concatenate\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, Dropout, SpatialDropout1D, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import RMSprop\n",
    "from tensorflow import keras\n",
    "from keras.utils import to_categorical\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.preprocessing import binarize, LabelEncoder, MinMaxScaler\n",
    "# import emoji\n",
    "!pip install contractions\n",
    "import contractions\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "from os.path import join, exists\n",
    "from keras.utils.data_utils import get_file\n",
    "import sys\n",
    "import time\n",
    "from imblearn.over_sampling import RandomOverSampler,SMOTE, ADASYN\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from nltk.stem import SnowballStemmer\n",
    "# from tqdm import tqdm\n",
    "# tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "import gensim\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import re\n",
    "from sklearn import neighbors\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract only words, by excluding punctuation and other symbols\n",
    "def clean_data(review):\n",
    "    review = review.lower() # convert text to lower-case\n",
    "    review = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', review) # remove URLs\n",
    "    review = re.sub('@[^\\s]+', 'AT_USER', review) # remove usernames\n",
    "    review = re.sub(r'#([^\\s]+)', r'\\1', review) # remove the # in #hashtag\n",
    "    review = re.sub(r'\\W', ' ', review) # Remove all the special characters\n",
    "    review= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', review)  # remove all single characters  \n",
    "    review = re.sub(r'\\^[a-zA-Z]\\s+', ' ', review)  # Remove single characters from the start\n",
    "    review = re.sub(r'\\s+', ' ', review, flags=re.I)# Substituting multiple spaces with single space\n",
    "    review =  re.sub(r\"\\d\", \"\", review) #Removing Digits\n",
    "    review = re.sub(r\"^\\s+\", \"\", review) #Removing Spaces from Start and End\n",
    "    review = contractions.fix(review) #to change words like \"don't\" to \"do not\"\n",
    "    review = re.sub(\"([^\\x00-\\x7F])+\",\" \",review) #removing non-english words\n",
    "#     review = word_tokenize(review) \n",
    "#     review = remove_stop_words(review) #remove stop words\n",
    "#     review = ' '.join(review)\n",
    "    return review\n",
    "\n",
    "def confusion_matrix(cnf_matrix):    \n",
    "    class_names=[0,1] # name  of classes\n",
    "    fig, ax = plt.subplots()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "    sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "    ax.xaxis.set_label_position(\"top\")\n",
    "    plt.tight_layout()\n",
    "    plt.title('Confusion matrix', y=1.1)\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    return \n",
    "def plotting_df(history):\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f0NWfafZTw5F"
   },
   "outputs": [],
   "source": [
    "##Dataset Class extends Pytorch inbuilt Dataset Class\n",
    "\n",
    "\n",
    "class Arizona(Dataset):\n",
    "  def __init__(self, file_path, transform=None):\n",
    "        self.data = file_path\n",
    "        self.transform = transform\n",
    "        self.targets = [a for a in self.data.iloc[:,1536]]\n",
    "        \n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "    \n",
    "  def __getitem__(self, index):\n",
    "    sample = self.data.iloc[index, :-1].values.reshape((1,1536))\n",
    "    label = self.targets[index]\n",
    "        \n",
    "    if self.transform is not None:\n",
    "      sample = self.transform(sample)\n",
    "            \n",
    "    return sample,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "56DbP6HATzqE"
   },
   "outputs": [],
   "source": [
    "## Neural Network architecture is named Network and this extends Pytorch inbuit nn.Module class\n",
    "\n",
    "class Network(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Network,self).__init__()\n",
    "    self.out = nn.Linear(in_features=1536,out_features=2)\n",
    "\n",
    "  def forward(self,t):\n",
    "    #implement forward pass method\n",
    "    t = t\n",
    "    t = t.reshape(-1, 1536) ##dont think this is necessary recheck\n",
    "    t = self.out(t)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Reading back lists with pickle\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('C:/python files/NLP/project/SBERT embeddings/train_q1.data', 'rb') as filehandle:\n",
    "    # read the data as binary data stream\n",
    "    train_q1 = pickle.load(filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Reading back lists with pickle\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('C:/python files/NLP/project/SBERT embeddings/train_q2.data', 'rb') as filehandle:\n",
    "    # read the data as binary data stream\n",
    "    train_q2 = pickle.load(filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Reading back lists with pickle\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('C:/python files/NLP/project/SBERT embeddings/test_q1.data', 'rb') as filehandle:\n",
    "    # read the data as binary data stream\n",
    "    test_q1 = pickle.load(filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Reading back lists with pickle\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('C:/python files/NLP/project/SBERT embeddings/test_q2.data', 'rb') as filehandle:\n",
    "    # read the data as binary data stream\n",
    "    test_q2 = pickle.load(filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Reading back lists with pickle\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('C:/python files/NLP/project/SBERT embeddings/label_test.data', 'rb') as filehandle:\n",
    "    # read the data as binary data stream\n",
    "    label_test = pickle.load(filehandle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Reading back lists with pickle\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('C:/python files/NLP/project/SBERT embeddings/label_train.txt', 'rb') as filehandle:\n",
    "    # read the data as binary data stream\n",
    "    label_train = pickle.load(filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "X_train = [list(itertools.chain.from_iterable([train_q1[i]] + [train_q2[i]])) for i in range(len(train_q1))]\n",
    "X_test = [list(itertools.chain.from_iterable([test_q1[i]] + [test_q2[i]])) for i in range(len(test_q1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = label_train\n",
    "y_test = label_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(X_test)\n",
    "test_df[\"labels\"] = label_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train= pd.DataFrame(X_train)\n",
    "train[\"labels\"] = label_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(train, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(327521, 1537)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36392, 1537)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40435, 1537)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df.labels \n",
    "y_test = test_df.labels\n",
    "y_val = val_df.labels\n",
    "X_train = train_df.drop([\"labels\"], axis=1)\n",
    "X_test = train_df.drop([\"labels\"], axis=1)\n",
    "X_val = train_df.drop([\"labels\"], axis=1)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df.labels \n",
    "X_train = train_df.drop([\"labels\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "363913"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersampleData(train_X, train_y):\n",
    "    rus = RandomUnderSampler(random_state=0)\n",
    "    X_resampled, y_resampled = rus.fit_resample(train_X, train_y)\n",
    "    return X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "X_resampled, y_resampled = undersampleData(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape: (268972, 1536)\n",
      "Train Data : \n",
      " 1    134486\n",
      "0    134486\n",
      "Name: 0, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAEKCAYAAADEovgeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAFNtJREFUeJzt3X+sX/V93/HnK3bJj24EEy5parOaNVY6wjoFroC1UoXCAoZ1MWphArXDSy15y0iWTtsKrFI9QZCSNhsLVYLkBQeMIgiiP/A2M88iYdFUfl1Cys9S35EObiH4UhNKGyXM2Xt/fD+3fDFf29fX/tyve/18SF99z3mfzznncyTLL51zPvfzTVUhSVJPbxt3ByRJS59hI0nqzrCRJHVn2EiSujNsJEndGTaSpO4MG0lSd4aNJKm7bmGTZEuS3UmeGLHt3ySpJCe19SS5Mcl0kseSnDHUdn2SXe2zfqh+ZpLH2z43Jkmrn5hkZ2u/M8mKXtcoSZqf9JpBIMnPAX8BbK2q04fqpwBfAn4KOLOqXk5yEfBJ4CLgbODzVXV2khOBKWASKOCRts8rSR4CPgU8AGwHbqyqe5L8JrCnqj6T5GpgRVVddbD+nnTSSbV69eojdv2SdCx45JFHXq6qiYO1W96rA1X1jSSrR2y6Afg14O6h2joGoVTAA0lOSPI+4FxgZ1XtAUiyE1ib5D7g+Kq6v9W3AhcD97RjnduOeytwH3DQsFm9ejVTU1OHdI2SdKxL8n/m025R39kk+Sjwp1X1h/tsWgk8P7Q+02oHqs+MqAO8t6peBGjfJx+gPxuTTCWZmp2dXcAVSZLmY9HCJsm7gF8HfmPU5hG1WkD9kFTV5qqarKrJiYmD3gVKkhZoMe9sfhI4FfjDJH8CrAK+meTHGNyZnDLUdhXwwkHqq0bUAV5qj+Bo37uP+JVIkg7JooVNVT1eVSdX1eqqWs0gMM6oqu8A24Ar2qi0c4BX2yOwHcD5SVa0UWXnAzvatteSnNNGoV3BG++AtgFzo9bW8+Z3Q5KkMeg59Pl24H7gA0lmkmw4QPPtwLPANPCfgX8B0AYGXAc83D7Xzg0WAD7OYFTbNPC/GQwOAPgM8JEku4CPtHVJ0hh1G/r8183k5GQ5Gk2SDk2SR6pq8mDtnEFAktSdYSNJ6s6wkSR1120GgWPRmf9267i7oKPQI791xbi7wHPX/t1xd0FHob/1G48v2rm8s5EkdWfYSJK6M2wkSd0ZNpKk7gwbSVJ3ho0kqTvDRpLUnWEjSerOsJEkdWfYSJK6M2wkSd0ZNpKk7gwbSVJ3ho0kqTvDRpLUnWEjSerOsJEkddctbJJsSbI7yRNDtd9K8kdJHkvye0lOGNp2TZLpJM8kuWCovrbVppNcPVQ/NcmDSXYl+WqS41r97W19um1f3esaJUnz0/PO5hZg7T61ncDpVfXTwB8D1wAkOQ24DPhg2+eLSZYlWQZ8AbgQOA24vLUF+CxwQ1WtAV4BNrT6BuCVqno/cENrJ0kao25hU1XfAPbsU/sfVbW3rT4ArGrL64A7quoHVfVtYBo4q32mq+rZqnoduANYlyTAh4G72v63AhcPHevWtnwXcF5rL0kak3G+s/kV4J62vBJ4fmjbTKvtr/4e4LtDwTVXf9Ox2vZXW3tJ0piMJWyS/DqwF/jKXGlEs1pA/UDHGtWPjUmmkkzNzs4euNOSpAVb9LBJsh74eeCXqmouBGaAU4aarQJeOED9ZeCEJMv3qb/pWG37u9nncd6cqtpcVZNVNTkxMXG4lyZJ2o9FDZska4GrgI9W1feGNm0DLmsjyU4F1gAPAQ8Da9rIs+MYDCLY1kLq68Albf/1wN1Dx1rfli8BvjYUapKkMVh+8CYLk+R24FzgpCQzwCYGo8/eDuxs7+wfqKp/XlVPJrkTeIrB47Urq+qH7TifAHYAy4AtVfVkO8VVwB1JPg08Ctzc6jcDtyWZZnBHc1mva5QkzU+3sKmqy0eUbx5Rm2t/PXD9iPp2YPuI+rMMRqvtW/8+cOkhdVaS1JUzCEiSujNsJEndGTaSpO4MG0lSd4aNJKk7w0aS1J1hI0nqzrCRJHVn2EiSujNsJEndGTaSpO4MG0lSd4aNJKk7w0aS1J1hI0nqzrCRJHVn2EiSujNsJEndGTaSpO4MG0lSd4aNJKk7w0aS1F23sEmyJcnuJE8M1U5MsjPJrva9otWT5MYk00keS3LG0D7rW/tdSdYP1c9M8njb58YkOdA5JEnj0/PO5hZg7T61q4F7q2oNcG9bB7gQWNM+G4GbYBAcwCbgbOAsYNNQeNzU2s7tt/Yg55AkjUm3sKmqbwB79imvA25ty7cCFw/Vt9bAA8AJSd4HXADsrKo9VfUKsBNY27YdX1X3V1UBW/c51qhzSJLGZLHf2by3ql4EaN8nt/pK4PmhdjOtdqD6zIj6gc4hSRqTo2WAQEbUagH1QztpsjHJVJKp2dnZQ91dkjRPix02L7VHYLTv3a0+A5wy1G4V8MJB6qtG1A90jreoqs1VNVlVkxMTEwu+KEnSgS122GwD5kaUrQfuHqpf0UalnQO82h6B7QDOT7KiDQw4H9jRtr2W5Jw2Cu2KfY416hySpDFZ3uvASW4HzgVOSjLDYFTZZ4A7k2wAngMubc23AxcB08D3gI8BVNWeJNcBD7d211bV3KCDjzMY8fZO4J724QDnkCSNSbewqarL97PpvBFtC7hyP8fZAmwZUZ8CTh9R/7NR55Akjc/RMkBAkrSEGTaSpO4MG0lSd4aNJKk7w0aS1J1hI0nqzrCRJHVn2EiSujNsJEndGTaSpO4MG0lSd4aNJKk7w0aS1J1hI0nqzrCRJHVn2EiSujNsJEndGTaSpO4MG0lSd4aNJKk7w0aS1J1hI0nqbixhk+RfJXkyyRNJbk/yjiSnJnkwya4kX01yXGv79rY+3bavHjrONa3+TJILhuprW206ydWLf4WSpGGLHjZJVgL/EpisqtOBZcBlwGeBG6pqDfAKsKHtsgF4pareD9zQ2pHktLbfB4G1wBeTLEuyDPgCcCFwGnB5aytJGpNxPUZbDrwzyXLgXcCLwIeBu9r2W4GL2/K6tk7bfl6StPodVfWDqvo2MA2c1T7TVfVsVb0O3NHaSpLGZNHDpqr+FPgc8ByDkHkVeAT4blXtbc1mgJVteSXwfNt3b2v/nuH6Pvvsry5JGpNxPEZbweBO41Tgx4EfZfDIa181t8t+th1qfVRfNiaZSjI1Ozt7sK5LkhZoHI/R/gHw7aqarar/C/wu8DPACe2xGsAq4IW2PAOcAtC2vxvYM1zfZ5/91d+iqjZX1WRVTU5MTByJa5MkjTCvsEly73xq8/QccE6Sd7V3L+cBTwFfBy5pbdYDd7flbW2dtv1rVVWtflkbrXYqsAZ4CHgYWNNGtx3HYBDBtgX2VZJ0BCw/0MYk72DwAv+k9vhr7hHV8QwegR2yqnowyV3AN4G9wKPAZuC/AXck+XSr3dx2uRm4Lck0gzuay9pxnkxyJ4Og2gtcWVU/bP3+BLCDwUi3LVX15EL6Kkk6Mg4YNsA/A36VQbA8whth8+cMhhcvSFVtAjbtU36WwUiyfdt+H7h0P8e5Hrh+RH07sH2h/ZMkHVkHDJuq+jzw+SSfrKrfXqQ+SZKWmIPd2QBQVb+d5GeA1cP7VNXWTv2SJC0h8wqbJLcBPwl8C/hhKxdg2EiSDmpeYQNMAqe1UWCSJB2S+f6dzRPAj/XsiCRp6Zrvnc1JwFNJHgJ+MFesqo926ZUkaUmZb9j8+56dkCQtbfMdjfY/e3dEkrR0zXc02mu8MZnlccCPAH9ZVcf36pgkaemY753N3xxeT3IxI/7aX5KkURY063NV/T6DHzuTJOmg5vsY7ReGVt/G4O9u/JsbSdK8zHc02j8aWt4L/An+1LIkaZ7m+87mY707Iklauub742mrkvxekt1JXkryO0lW9e6cJGlpmO8AgS8z+LXLHwdWAv+l1SRJOqj5hs1EVX25qva2zy3ARMd+SZKWkPmGzctJfjnJsvb5ZeDPenZMkrR0zDdsfgX4x8B3gBeBSwAHDUiS5mW+Q5+vA9ZX1SsASU4EPscghCRJOqD53tn89FzQAFTVHuBDfbokSVpq5hs2b0uyYm6l3dnM965IknSMm2/Y/AfgD5Jcl+Ra4A+A31zoSZOckOSuJH+U5Okkfz/JiUl2JtnVvle0tklyY5LpJI8lOWPoOOtb+11J1g/Vz0zyeNvnxiRZaF8lSYdvXmFTVVuBXwReAmaBX6iq2w7jvJ8H/ntV/RTw94CngauBe6tqDXBvWwe4EFjTPhuBm+Cv7q42AWczmIF609Dd102t7dx+aw+jr5KkwzTvR2FV9RTw1OGeMMnxwM8B/7Qd93Xg9STrgHNbs1uB+4CrGMzBtrWqCnig3RW9r7Xd2d4fkWQnsDbJfcDxVXV/q28FLgbuOdy+S5IWZkE/MXCY/jaDu6MvJ3k0yZeS/Cjw3qp6EaB9n9zarwSeH9p/ptUOVJ8ZUX+LJBuTTCWZmp2dPfwrkySNNI6wWQ6cAdxUVR8C/pI3HpmNMup9Sy2g/tZi1eaqmqyqyYkJJ0SQpF7GETYzwExVPdjW72IQPi+1x2O0791D7U8Z2n8V8MJB6qtG1CVJY7LoYVNV3wGeT/KBVjqPwbugbcDciLL1wN1teRtwRRuVdg7wanvMtgM4P8mKNjDgfGBH2/ZaknPaKLQrho4lSRqDcf2tzCeBryQ5DniWwdQ3bwPuTLIBeA64tLXdDlwETAPfa22pqj1JrgMebu2unRssAHwcuAV4J4OBAQ4OkKQxGkvYVNW3GPy09L7OG9G2gCv3c5wtwJYR9Sng9MPspiTpCBnHOxtJ0jHGsJEkdWfYSJK6M2wkSd0ZNpKk7gwbSVJ3ho0kqTvDRpLUnWEjSerOsJEkdWfYSJK6M2wkSd0ZNpKk7gwbSVJ3ho0kqTvDRpLUnWEjSerOsJEkdWfYSJK6M2wkSd0ZNpKk7gwbSVJ3YwubJMuSPJrkv7b1U5M8mGRXkq8mOa7V397Wp9v21UPHuKbVn0lywVB9batNJ7l6sa9NkvRm47yz+RTw9ND6Z4EbqmoN8AqwodU3AK9U1fuBG1o7kpwGXAZ8EFgLfLEF2DLgC8CFwGnA5a2tJGlMxhI2SVYB/xD4UlsP8GHgrtbkVuDitryurdO2n9farwPuqKofVNW3gWngrPaZrqpnq+p14I7WVpI0JuO6s/lPwK8B/6+tvwf4blXtbeszwMq2vBJ4HqBtf7W1/6v6Pvvsr/4WSTYmmUoyNTs7e7jXJEnaj0UPmyQ/D+yuqkeGyyOa1kG2HWr9rcWqzVU1WVWTExMTB+i1JOlwLB/DOX8W+GiSi4B3AMczuNM5IcnydveyCnihtZ8BTgFmkiwH3g3sGarPGd5nf3VJ0hgs+p1NVV1TVauqajWDF/xfq6pfAr4OXNKarQfubsvb2jpt+9eqqlr9sjZa7VRgDfAQ8DCwpo1uO66dY9siXJokaT/GcWezP1cBdyT5NPAocHOr3wzclmSawR3NZQBV9WSSO4GngL3AlVX1Q4AknwB2AMuALVX15KJeiSTpTcYaNlV1H3BfW36WwUiyfdt8H7h0P/tfD1w/or4d2H4EuypJOgzOICBJ6s6wkSR1Z9hIkrozbCRJ3Rk2kqTuDBtJUneGjSSpO8NGktSdYSNJ6s6wkSR1Z9hIkrozbCRJ3Rk2kqTuDBtJUneGjSSpO8NGktSdYSNJ6s6wkSR1Z9hIkrozbCRJ3Rk2kqTuFj1skpyS5OtJnk7yZJJPtfqJSXYm2dW+V7R6ktyYZDrJY0nOGDrW+tZ+V5L1Q/Uzkzze9rkxSRb7OiVJbxjHnc1e4F9X1d8BzgGuTHIacDVwb1WtAe5t6wAXAmvaZyNwEwzCCdgEnA2cBWyaC6jWZuPQfmsX4bokSfux6GFTVS9W1Tfb8mvA08BKYB1wa2t2K3BxW14HbK2BB4ATkrwPuADYWVV7quoVYCewtm07vqrur6oCtg4dS5I0BmN9Z5NkNfAh4EHgvVX1IgwCCTi5NVsJPD+020yrHag+M6IuSRqTsYVNkr8B/A7wq1X15wdqOqJWC6iP6sPGJFNJpmZnZw/WZUnSAo0lbJL8CIOg+UpV/W4rv9QegdG+d7f6DHDK0O6rgBcOUl81ov4WVbW5qiaranJiYuLwLkqStF/jGI0W4Gbg6ar6j0ObtgFzI8rWA3cP1a9oo9LOAV5tj9l2AOcnWdEGBpwP7GjbXktyTjvXFUPHkiSNwfIxnPNngX8CPJ7kW63274DPAHcm2QA8B1zatm0HLgKmge8BHwOoqj1JrgMebu2urao9bfnjwC3AO4F72keSNCaLHjZV9b8Y/V4F4LwR7Qu4cj/H2gJsGVGfAk4/jG5Kko4gZxCQJHVn2EiSujNsJEndGTaSpO4MG0lSd4aNJKk7w0aS1J1hI0nqzrCRJHVn2EiSujNsJEndGTaSpO4MG0lSd4aNJKk7w0aS1J1hI0nqzrCRJHVn2EiSujNsJEndGTaSpO4MG0lSd4aNJKm7JRs2SdYmeSbJdJKrx90fSTqWLcmwSbIM+AJwIXAacHmS08bbK0k6di3JsAHOAqar6tmqeh24A1g35j5J0jFrqYbNSuD5ofWZVpMkjcHycXegk4yo1VsaJRuBjW31L5I807VXx5aTgJfH3YmjQT63ftxd0Jv5b3POplH/VR6yn5hPo6UaNjPAKUPrq4AX9m1UVZuBzYvVqWNJkqmqmhx3P6R9+W9zPJbqY7SHgTVJTk1yHHAZsG3MfZKkY9aSvLOpqr1JPgHsAJYBW6rqyTF3S5KOWUsybACqajuwfdz9OIb5eFJHK/9tjkGq3vLeXJKkI2qpvrORJB1FDBsdUU4TpKNVki1Jdid5Ytx9ORYZNjpinCZIR7lbgLXj7sSxyrDRkeQ0QTpqVdU3gD3j7sexyrDRkeQ0QZJGMmx0JM1rmiBJxx7DRkfSvKYJknTsMWx0JDlNkKSRDBsdMVW1F5ibJuhp4E6nCdLRIsntwP3AB5LMJNkw7j4dS5xBQJLUnXc2kqTuDBtJUneGjSSpO8NGktSdYSNJ6s6wkY5izqKtpcKhz9JRqs2i/cfARxjMzvAwcHlVPTXWjkkL4J2NdPRyFm0tGYaNdPRyFm0tGYaNdPRyFm0tGYaNdPRyFm0tGYaNdPRyFm0tGcvH3QFJo1XV3iRzs2gvA7Y4i7b+unLosySpOx+jSZK6M2wkSd0ZNpKk7gwbSVJ3ho0kqTvDRpLUnWEjSerOsJEkdff/AauvI+umWl1QAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Train Data Shape:\",X_resampled.shape)\n",
    "y = pd.DataFrame(y_resampled)\n",
    "print(\"Train Data : \\n\",y[0].value_counts())\n",
    "\n",
    "sns.countplot(y[0],label=\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_sampled = pd.DataFrame(X_resampled)\n",
    "train_df_sampled[\"labels\"] = y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wADauMXwT9WY"
   },
   "outputs": [],
   "source": [
    "# train_df = df_for_task1(final_sent_embedding,train)\n",
    "\n",
    "# val_df = df_for_task1(final_sent_embedding_val,validation)\n",
    "\n",
    "# test_df = df_for_task1(final_sent_embedding_test,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l5CoxINyUBRV"
   },
   "outputs": [],
   "source": [
    "train_dataset = Arizona(train_df_sampled,transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=2048)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = Arizona(test_df,transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=2048)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = Arizona(val_df,transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset,batch_size=2048)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fbckVxnyUE3q"
   },
   "outputs": [],
   "source": [
    "#Creating an instance of Network class which is our model here\n",
    "\n",
    "## BS : 1024, optimizer : Adam , learning rate = 0.01 , Epochs : 10\n",
    "\n",
    "variant = Network()\n",
    "\n",
    "## Creating an optimizer instance and passing it the learning rate\n",
    "\n",
    "optimizer = optim.Adam(variant.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5jjPHdUbTF1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved model now\n",
      "epoch: 0.0000  loss: 49.5688  loss_val: 9.3138  train_acc: 1.0000  val_acc: 0.3915\n",
      "epoch: 1.0000  loss: 278.3271  loss_val: 9.6057  train_acc: 1.0000  val_acc: 0.3915\n",
      "epoch: 2.0000  loss: 296.9583  loss_val: 9.0266  train_acc: 1.0000  val_acc: 0.3915\n"
     ]
    }
   ],
   "source": [
    "## Training and storing the model which performed the best on validation set\n",
    "\n",
    "## Storing loss and accuracy to plot corresponding graphs\n",
    "\n",
    "\n",
    "for batch_val in val_loader:\n",
    "  images_val,labels_val = batch_val\n",
    "\n",
    "for batch_test in test_loader:\n",
    "  images_test,labels_test = batch_test\n",
    "\n",
    "for batch_total in train_loader:\n",
    "  images_total,labels_total = batch_total\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_accuracy = []\n",
    "val_accuracy = []\n",
    "max_acc_val = 0\n",
    "for epoch in range(25):\n",
    "  total_loss = 0\n",
    "  loss_val = 0\n",
    "  for batch in train_loader:\n",
    "    images,labels = batch\n",
    "    preds = variant(images.float())\n",
    "\n",
    "    loss = F.cross_entropy(preds,labels)\n",
    "    \n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "  preds_val = variant(images_val.float())\n",
    "  loss_val = F.cross_entropy(preds_val,labels_val)\n",
    "  loss_val = loss_val.item()\n",
    "  train_loss.append(total_loss)\n",
    "  val_loss.append(loss_val)\n",
    "  train_acc = preds.argmax(dim=1).eq(labels).sum().item()/len(images)\n",
    "  train_accuracy.append(train_acc)\n",
    "  val_acc = preds_val.argmax(dim=1).eq(labels_val).sum().item()/len(images_val)\n",
    "  val_accuracy.append(val_acc)\n",
    "  if val_acc > max_acc_val: \n",
    "    max_acc_val = val_acc\n",
    "#     torch.save(variant.state_dict(), \"/best_variant_model.pth\")\n",
    "    print(\"saved model now\")\n",
    "  print(\"epoch: {:.4f}\".format(epoch) ,\" loss: {:.4f}\".format(total_loss),\" loss_val: {:.4f}\".format(loss_val),\" train_acc: {:.4f}\".format(train_acc),\" val_acc: {:.4f}\".format(val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting(loss_val,total_loss,train_acc,val_acc):\n",
    "#     print(loss_val)\n",
    "    plt.plot(train_acc)\n",
    "    plt.plot(val_acc)\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # summarize history for loss\n",
    "    plt.plot(total_loss)\n",
    "    plt.plot(loss_val)\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loss = []\n",
    "# val_loss = []\n",
    "# train_accuracy = []\n",
    "# val_accuracy =\n",
    "plotting(val_loss,train_loss,train_accuracy,val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_total = variant1(images_total.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(cnf_matrix):    \n",
    "    class_names=[0,1] # name  of classes\n",
    "    fig, ax = plt.subplots()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "    sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "    ax.xaxis.set_label_position(\"top\")\n",
    "    plt.tight_layout()\n",
    "    plt.title('Confusion matrix', y=1.1)\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4IilR4OGUSEP"
   },
   "outputs": [],
   "source": [
    "# ## Classification Report for a single batch of samples from train set\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "# target_names = ['CONTRADICTION', 'NEUTRAL', 'ENTAILMENT']\n",
    "# print(classification_report(labels.numpy(),preds.argmax(dim=1).numpy(), target_names=target_names))\n",
    "\n",
    "\n",
    "# ## Passing complete train loader to the model for train predictions\n",
    "\n",
    "preds_total = variant(images_total.float())\n",
    "\n",
    "\n",
    "# Classification Report for Complete Train set\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = [0,1]\n",
    "print(classification_report(labels_total,preds_total.argmax(dim=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = variant(images_test.float())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# target_names = [0,1]\n",
    "print(classification_report(labels_test,preds_test.argmax(dim=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = metrics.confusion_matrix(labels_test,preds_test.argmax(dim=1))\n",
    "confusion_matrix(cnf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "LR_Model_Skelton.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
